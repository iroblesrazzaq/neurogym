{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent on the Perceptual Decision Making Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from neurogym.wrappers.monitor import Monitor\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from pathlib import Path\n",
    "from neurogym.utils.ngym_random import TruncExp\n",
    "from neurogym.wrappers.reaction_time import ReactionTime\n",
    "from neurogym.wrappers.pass_action import PassAction\n",
    "from neurogym.wrappers.pass_reward import PassReward\n",
    "from neurogym.wrappers.side_bias import SideBias\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "from plotting import plot_env\n",
    "from utilities import compute_folder_metrics, collect_aggregate_pdm, plot_aggregate_pdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Configuration and Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "EVAL_TRIALS = 1000\n",
    "dt = 100\n",
    "dim_ring = 2  # Number of choices in the ring representation\n",
    "abort = False  # Whether to allow aborting the trial if the agent does not fixate # TODO: add this as attribute to the environment\n",
    "rewards = {\n",
    "    \"abort\": -0.1,\n",
    "    \"correct\": +1.0,\n",
    "    \"fail\": 0.0\n",
    "}\n",
    "timing = {\n",
    "    \"fixation\": TruncExp(600, 400, 700),\n",
    "    \"stimulus\": 2000,\n",
    "    \"delay\": 0,\n",
    "    \"decision\": 100,\n",
    "}\n",
    "sigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation\n",
    "\n",
    "kwargs = {\n",
    "    \"dt\": dt,\n",
    "    \"dim_ring\": dim_ring,\n",
    "    \"rewards\": rewards,\n",
    "    \"timing\": timing,\n",
    "    \"sigma\": sigma,\n",
    "    \"abort\": abort,\n",
    "}\n",
    "block_dur = (20, 100)\n",
    "probs = [[0.2, 0.8], [0.8, 0.2]]\n",
    "\n",
    "# Create and wrap the environment\n",
    "task = \"PerceptualDecisionMaking-v0\"\n",
    "\n",
    "env = gym.make(task, **kwargs)\n",
    "env = ReactionTime(env, end_on_stimulus=True)\n",
    "env = PassReward(env)\n",
    "env = PassAction(env)\n",
    "env = SideBias(env, probs=probs, block_dur=block_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained Environment Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize example trials\n",
    "fig = plot_env(\n",
    "    env,\n",
    "    name='IBL',\n",
    "    ob_traces=[\n",
    "        'Fixation',\n",
    "        'Stim 1',\n",
    "        'Stim 2',\n",
    "        'PassReward',\n",
    "        'PassAction'\n",
    "    ],\n",
    "    num_trials=15,\n",
    "    plot_performance=True,\n",
    "    fig_kwargs={'figsize': (9, 5)},\n",
    ")\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "fig.savefig(\"untrained_env.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Evaluate performance of the environment before training\n",
    "eval_monitor = Monitor(\n",
    "    env\n",
    ")\n",
    "print(\"\\nEvaluating random policy performance...\")\n",
    "metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS)\n",
    "print(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {metrics['mean_reward']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TRAIN_TRIALS = 10000\n",
    "EVAL_TRIALS = 1000\n",
    "NUM_MODELS = 10\n",
    "\n",
    "# Get average timesteps per trial once\n",
    "avg_timesteps = 7 # observed\n",
    "total_timesteps = TRAIN_TRIALS * avg_timesteps\n",
    "trials_per_figure = 10\n",
    "steps_per_figure = trials_per_figure * avg_timesteps\n",
    "trials_per_batch = 64\n",
    "n_steps = avg_timesteps * trials_per_batch\n",
    "batch_size = 32\n",
    "\n",
    "# Policy configuration\n",
    "policy_kwargs = {\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"n_lstm_layers\": 1,\n",
    "    \"shared_lstm\": True,\n",
    "    \"enable_critic_lstm\": False,\n",
    "}\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    print(f\"\\n=== Training model {i + 1}/{NUM_MODELS} ===\")\n",
    "\n",
    "    # Set seed\n",
    "    seed = i\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "    # Create new Monitor instance with separate directory\n",
    "    train_monitor = Monitor(\n",
    "        env,\n",
    "        trigger=\"trial\",\n",
    "        interval=1000,\n",
    "        plot_create=True,\n",
    "        plot_steps=steps_per_figure,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Vectorize the monitored environment\n",
    "    env_vec = DummyVecEnv([lambda: train_monitor])\n",
    "\n",
    "    # Create model\n",
    "    model = RecurrentPPO(\n",
    "        \"MlpLstmPolicy\",\n",
    "        env_vec,\n",
    "        learning_rate=5e-4,\n",
    "        n_steps=n_steps,\n",
    "        batch_size=batch_size,\n",
    "        ent_coef=0.01,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        seed=seed,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model.learn(total_timesteps=total_timesteps, log_interval=total_timesteps // 10)\n",
    "\n",
    "    # Save model in monitor directory with custom filename\n",
    "    model_path = f\"rl_model_{i+1}.zip\"\n",
    "    model.save(train_monitor.save_dir / model_path)\n",
    "\n",
    "    # Clean up environment\n",
    "    env_vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Training and Load the Latest Saved RL Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# ONLY TO AVOID RERUNNING THE TRAINING\n",
    "\n",
    "# Constants\n",
    "TRAIN_TRIALS = 10000\n",
    "EVAL_TRIALS = 1000\n",
    "NUM_MODELS = 10\n",
    "\n",
    "# Get average timesteps per trial once\n",
    "avg_timesteps = 7 # observed\n",
    "total_timesteps = TRAIN_TRIALS * avg_timesteps\n",
    "trials_per_figure = 10\n",
    "steps_per_figure = trials_per_figure * avg_timesteps\n",
    "trials_per_batch = 64\n",
    "n_steps = avg_timesteps * trials_per_batch\n",
    "batch_size = 32\n",
    "\n",
    "# Policy configuration\n",
    "policy_kwargs = {\n",
    "    \"lstm_hidden_size\": 64,\n",
    "    \"n_lstm_layers\": 1,\n",
    "    \"shared_lstm\": True,\n",
    "    \"enable_critic_lstm\": False,\n",
    "}\n",
    "\n",
    "train_monitor = Monitor(\n",
    "    env,\n",
    "    trigger=\"trial\",\n",
    "    interval=1000,\n",
    "    plot_create=True,\n",
    "    plot_steps=steps_per_figure,\n",
    "    verbose=True,\n",
    ")\n",
    "env_vec = DummyVecEnv([lambda: train_monitor])\n",
    "train_monitor.save_dir = Path(\"/Users/giuliacrocioni/Desktop/docs/eScience/projects/ANNUBeS/paper/figure_2/runs/PerceptualDecisionMaking/2025-06-05_14-31-08\")\n",
    "model_path = \"rl_model_1.zip\"\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Trained RL Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = RecurrentPPO.load(train_monitor.save_dir / model_path)\n",
    "\n",
    "# Plot example trials with trained agent\n",
    "fig = plot_env(\n",
    "    env_vec,\n",
    "    name='IBL (trained)',\n",
    "    ob_traces=[\n",
    "        'Fixation',\n",
    "        'Stim 1',\n",
    "        'Stim 2',\n",
    "        'PassReward',\n",
    "        'PassAction'\n",
    "    ],\n",
    "    num_trials=5,\n",
    "    model=loaded_model,\n",
    "    plot_performance=True,\n",
    "    fig_kwargs={'figsize': (9, 5)}\n",
    ")\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "fig.savefig(\"trained_env.pdf\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Trained RL Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of the last trained model\n",
    "print(\"\\nEvaluating trained model performance...\")\n",
    "rl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=loaded_model)\n",
    "print(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")\n",
    "\n",
    "fig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False)\n",
    "\n",
    "ax = fig.axes[0]\n",
    "ax.set_title(\"\")\n",
    "fig._suptitle.remove()\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "\n",
    "fig.savefig(\"training_history.pdf\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training History for All RL Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for ALL models\n",
    "data_base_dir = train_monitor.config.local_dir / train_monitor.config.env.name\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "all_rewards = []\n",
    "all_performances = []\n",
    "all_indices = []\n",
    "\n",
    "for data_dir in sorted(data_base_dir.glob(\"*\")):\n",
    "    if not data_dir.is_dir():\n",
    "        continue\n",
    "    data_files = list(data_dir.glob(\"*.npz\"))\n",
    "    if not data_files:\n",
    "        continue\n",
    "    file_indices, cum_rewards, performances = compute_folder_metrics(data_dir)\n",
    "    ax.plot(file_indices, performances, color=\"lightblue\", linewidth=1, alpha=0.5)\n",
    "    all_performances.append(performances)\n",
    "    ax.plot(file_indices, cum_rewards, color=\"lightcoral\", linewidth=1, alpha=0.5)\n",
    "    all_rewards.append(cum_rewards)\n",
    "    all_indices.append(file_indices)\n",
    "\n",
    "# Plot average curves\n",
    "avg_rewards = np.mean(np.array(all_rewards), axis=0)\n",
    "avg_performances = np.mean(np.array(all_performances), axis=0)\n",
    "common_indices = all_indices[0]\n",
    "\n",
    "ax.plot(common_indices, avg_performances, \"o-\", color=\"blue\", label=\"Average Accuracy\", linewidth=2)\n",
    "ax.plot(common_indices, avg_rewards, \"o-\", color=\"red\", label=\"Average Reward\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Trials\")\n",
    "ax.set_ylabel(\"Accuracy / Reward\")\n",
    "ax.set_ylim(-0.05, 1)\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1, 0.2))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.8)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "\n",
    "fig.savefig(\"training_history_all_models.pdf\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All RL Models and Store Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all trained RL models and store performance\n",
    "models_base_dir = train_monitor.config.local_dir / train_monitor.config.env.name\n",
    "\n",
    "rl_mean_performance = []\n",
    "\n",
    "for model_dir in sorted(models_base_dir.glob(\"*\")):\n",
    "    if not model_dir.is_dir():\n",
    "        continue\n",
    "    model_files = list(model_dir.glob(\"*.zip\"))\n",
    "    if not model_files:\n",
    "        continue\n",
    "    model_path = model_files[0]\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "    model = RecurrentPPO.load(model_path)\n",
    "    metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=model)\n",
    "    rl_mean_performance.append(metrics['mean_performance'])\n",
    "    print(f\"{model_path} accuracy: {metrics['mean_performance']:.4f}\")\n",
    "\n",
    "# Print average performance\n",
    "mean_performance = np.mean(rl_mean_performance)\n",
    "std_performance = np.std(rl_mean_performance)\n",
    "print(f\"\\nAverage RL model performance: {mean_performance:.4f} ± {std_performance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Models Psychometric Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "models_base_dir = train_monitor.config.local_dir / train_monitor.config.env.name\n",
    "for model_dir in sorted(models_base_dir.glob(\"*\")):\n",
    "    if model_dir.is_dir():\n",
    "        model_files = list(model_dir.glob(\"*.zip\"))\n",
    "        if model_files:\n",
    "            model = RecurrentPPO.load(model_files[0])\n",
    "            eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=model)\n",
    "            data_list.append(eval_monitor.data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = collect_aggregate_pdm(data_list, probs)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "\n",
    "for i, blk in enumerate(range(len(probs))):\n",
    "    plot_aggregate_pdm(ax, blk, aggregate)\n",
    "\n",
    "ax.set_ylabel(\"P(Right)\", fontsize=12)\n",
    "\n",
    "# Styling\n",
    "plt.tight_layout()\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.legend(loc=\"center right\", bbox_to_anchor=(1, 0.2))\n",
    "\n",
    "plt.savefig(\"psychometric_avg.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
